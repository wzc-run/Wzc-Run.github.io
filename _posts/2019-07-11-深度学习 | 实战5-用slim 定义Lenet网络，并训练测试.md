---
layout:     post
title:      深度学习 | 实战5-用slim 定义Lenet网络，并训练测试
subtitle:
date:       2019-07-11
author:     JoselynZhao
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - Deep Learning
    - Python
    - TensorFlow
    - slim

---
[Github源码](https://github.com/zhaojing1995/DeepLearning.Advanceing/tree/master/DL_5/work)
## 要求
Slim Lenet


用slim 定义Lenet网络，并训练测试。


要求：

1. 将Lenet 单独定义到Lenet.py 文件

可以定义为一个函数，例如：


def lenet(images):


2. 用with slim.arg_scope .....: 去管理 lenet中所有操作的默认参数， 例如activation_fn, weights_initializer, 等。。。


3. 编写mnist_train.py脚本，训练slim定义的lenet做MNIST字符分类。


这里可以不要求用slim中的slim.learning.train，因为这个涉及转换数据为TFRecord以及用队列读取等复杂操作去自动取数据。

大家可以还用以前的sess.run 去训练模型。


提交：

文档、源码。文档包括训练截屏、结果图片等，能帮助老师快速判断结果是否正确。

必须按照要求去完成作业。


## 实验与结果
运行截图
图 1 开始运行，每500轮输出一个结果

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717183833791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)
图 2 运行结束。因为实验平台是Macbook air，没有GPU用以训练，CPU训练很慢很烧电脑，这里就只开了10000轮的训练，但应该对训练结果的影响不大。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717183840203.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)

图 3  实验结果。 最后训练出来的模型在测试集上的准确率是0.9887.
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717183845460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)

参数设置
图 4 迭代10000轮，batch_size是64，学习率是0.1

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717183856256.png)

## 代码展示
### LENET

```py
def lenet(image):
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
                        activation_fn=tf.nn.relu,
                        weights_initializer=tf.truncated_normal_initializer(0.0,0.1), #mu，sigma
                        weights_regularizer=slim.l2_regularizer(0.1)):
        net = slim.conv2d(image, 6, [5, 5], stride=1, padding="VALID", scope="conv1")
        net = slim.max_pool2d(net, [2, 2], stride=2, padding="VALID", scope="pool1")
        net = slim.conv2d(net,16,[5,5],stride=1,padding = "VALID",scope ="conv2")
        net = slim.max_pool2d(net,[2,2],stride=2,padding="VALID",scope="pool2")
        net = slim.flatten(net,scope="flatten")
        net = slim.fully_connected(net,120, scope='fc1')
        net = slim.fully_connected(net,84, scope='fc2')
        net = slim.fully_connected(net,10,activation_fn=None, scope='fc3')
        return net

```

### mnist_train

```py
if __name__ =="__main__":
    mnist = input_data.read_data_sets('../../../data/mnist', one_hot=True)
    x_test = np.reshape(mnist.test.images, [-1, 28, 28, 1])
    x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)),'constant')  # print("Updated Image Shape: {}".format(X_train[0].shape))

    iteratons = 10000
    batch_size = 64
    lr = 0.1

    x = tf.placeholder(tf.float32, [None, 32, 32, 1])
    y_ = tf.placeholder(tf.float32, [None, 10])
    y = lenet(x)
    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))
    train_step = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cross_entropy)
    # 准确率
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))
    init = tf.global_variables_initializer()

    sess = tf.Session()
    sess.run(init)
    for i in range(iteratons):
        batch_xs,batch_ys = mnist.train.next_batch(batch_size)
        batch_xs = np.reshape(batch_xs, [-1, 28, 28, 1])
        batch_xs = np.pad(batch_xs, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant')
        sess.run([train_step,cross_entropy],feed_dict={x:batch_xs,y_:batch_ys})
        if i % 500 ==1:
            acc = sess.run(accuracy,feed_dict={x:x_test,y_:mnist.test.labels})
            print("%5d: accuracy is: %4f" % (i, acc))

    print('[accuracy,loss]:',sess.run([accuracy,cross_entropy],feed_dict={x:x_test,y_:mnist.test.labels}))

```

