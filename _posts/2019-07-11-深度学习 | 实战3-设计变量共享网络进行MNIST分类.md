---
layout:     post
title:      深度学习 | 实战3-设计变量共享网络进行MNIST分类
subtitle:
date:       2019-07-11
author:     JoselynZhao
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - Deep Learning
    - Python
    - TensorFlow

---


[Github源码](https://github.com/zhaojing1995/DeepLearning.Advanceing/tree/master/DL-3/work)
## 要求
设计变量共享网络进行MNIST分类：
网络结构如图所示：


![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717165244428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)


其将图片样本分为上下两半X1,X2；分别送入input1,input2。后续的两个路径的线性加权模块 X_W=X*W 共享一个变量 name='w'

整个分类模型可描述为 softmax( X_W(X1)+X_W(X2)+b)


模型及流程可以参考我们课件part1上最后的那个一层全连接分MNIST的代码例子


要求：1. 线性加权模块 X_W需定义为一个函数，在此函数中创建并共享变量W name='w'


函数X_W(X)只有一个输入参数X

W必须在X_W(X)中用get_variable定义


```py
def X_W(X)

	...
	
	return tf.matmul(X,W)

```

预期结果：


训练精度大概最后在0.85左右

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717165306527.png)


共享变量可视化：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717165317517.png)
提交：1. 文档（训练过程截图，训练、测试精度等）。2. 代码


## 实验与结果
X_W函数定义
图 1
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717165432246.png)
返回W是为了后面获得W的可视化图像。

训练截图
图 2
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717165444128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)

共迭代了10000轮，每500轮输出一次准确率结果。
W的图像输出 测试了很多种排版，最后选择了5*2，这样子看起来比较清晰直观。 

共享变量W的可视化结果
图 3
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190717165458214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05HVWV2ZXIxNQ==,size_16,color_FFFFFF,t_70)

## 源码展示

```py

if __name__ =="__main__":

    mnist = input_data.read_data_sets('../../../data/mnist', one_hot=True)
    tf.logging.set_verbosity(old_v)
    def X_W(x):
        with tf.variable_scope("X_W",reuse = tf.AUTO_REUSE):
            W = tf.get_variable("w",[392,10])
            y = tf.matmul(x,W)
            return W,y

    input1 = tf.placeholder(dtype='float',shape=[None,392])
    input2 = tf.placeholder(dtype='float',shape=[None,392])

    # x = tf.placeholder(dtype='float',shape=[None,784])
    # w = tf.Variable(tf.zeros([784,10]))
    b = tf.Variable(tf.zeros([10]))
    _,y1 = X_W(input1)
    weight,y2 = X_W(input2)
    y = tf.nn.softmax(y1+y2+b)
    y_ = tf.placeholder(dtype='float',shape=[None,10])

    cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
    train_step = tf.train.GradientDescentOptimizer(learning_rate=0.005).minimize(cross_entropy)
    #准确率
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))

    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)
    step = 10000
    batch_size = 64
    # loss_list = []
    for i in range(step):
        batch_xs,batch_ys = mnist.train.next_batch(batch_size)
        _,loss,w= sess.run([train_step,cross_entropy,weight],feed_dict={input1:batch_xs[:,0:392],input2:batch_xs[:,392:784],y_:batch_ys})
        if i % 500 ==1:
            acc = sess.run(accuracy,feed_dict={input1:mnist.test.images[:,0:392],input2:mnist.test.images[:,392:784],y_:mnist.test.labels})
            print("%5d: accuracy is: %4f" % (i, acc))

    print('[accuracy,loss]:',sess.run([accuracy,cross_entropy],feed_dict={input1:mnist.test.images[:, 0:392],input2:mnist.test.images[:, 392:784],y_:mnist.test.labels}))
    w = np.array(w)
    font1 = {'family': 'Times New Roman',
             'weight': 'normal',
             'size': 7,}
    matplotlib.rc('font', **font1)
    plt.figure()
    for i in range(10):
        weight = w[:,i]
        weight = np.reshape(weight,[14,28])
        plt.subplot(5,2,i)
        plt.title(i)
        plt.imshow(weight)
    plt.savefig('./save/weight.png')
    plt.show()
```


