---
layout:     post
title:      Deep Learning | 线性代数基础
subtitle:   深度学习中用到的线性代数基础
date:       2019-04-20
author:     JoselynZhao
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - Deep Learning
    - 线性代数
---

**导言**
>掌握好线性代数对于理解和从事机器学习算法相关工作是很有必要的，尤其对于深度学习算法而言。

# 标量、向量、矩阵和张量
## 标量scalar
一个标量就是一个单独的数，我们用**斜体**表示。
## 向量vector
一个向量就是一**列**数。
如果每个元素都属于R，并且该向量有n个元素，那么该向量属于**实属集R的n次笛卡尔乘积构成的集合**，即为$R^n$。
$$x = \begin{bmatrix}
x_1\\ 
x_2\\ 
...\\ 
x_n
\end{bmatrix}$$
我们把向量看做空间中的点，每个元素是不同坐标轴上的坐标。
指定$x_1,x_3,x_6$,我们定义结合S={1,3,6},然后写作$x_s$。
我们用符号-表示集合的补集中的索引。
比如$x_{-1}$表示x中除$x_{1}$外的所有元素。

## 矩阵matrix
矩阵是一个二维数组，其中的每一个元素由两个索引而确定。
$A_{i,:}$表示A中垂直坐标i上的一横排元素，也称为A的第i行（row）。

$f(A)_{i,j}$表示函数f作用在A上输出的矩阵的第i行j列元素。

## 张量tensor
张量用于讨论坐标超过两维的数组。
一般的，一个数组总的元素分布在若干维坐标的规则网格中，我们称之位**张量**。
张量A中坐标为(i,j,k)的元素记作$A_{i,j,k}$。

转置（transpose）是矩阵的重要操作之一。
矩阵的转置是以矩阵的对角线为轴的镜像，矩阵A的转置定义如下：
$$(A^T)_{i,j} = A_{j,i}$$

**矩阵加法**
只要矩阵的形状一样，我们可以把两个矩阵相加。即对应位置上的元素相加，比如C=A+B，其中$C_{i,j}= A_{i,j}+B_{i,j}$.

标量和矩阵**相乘**或**相加**时，只需要将其与矩阵的每个元素相乘或相加，比如D=a·B+c,其中$D_{i,j}=a·B_{i,j}+c$。

**矩阵与向量相加：**
C=A+b，其中$C_{i,j} = A_{i,j}+b_j$.
>也就是说矩阵的第j列加上$b_j$. (将向量b看做是横向的)

# 矩阵和向量相乘
矩阵乘积称为matrix product。
可以进行矩阵乘积操作的**前提**是: 矩阵A的列数和矩阵B的行数相等。
果然A的形状是$m \times n$, 矩阵B的形状是$n \times p$，那么矩阵A和矩阵B相乘得到的矩阵C的形状就是$m \times p$。

## 矩阵乘法定义
$$C_{i,j}=\sum_kA_{i,k}B_{k,j}$$
>横行乘竖列 得交点值

而两个矩阵中对应元素相乘称为**元素对应相乘（element-wise product）**或者**Hadamard乘积**，记作$A \bigodot  B$.

两个相同维数的向量x和y的**点积（dot product）**，可以看做是两个矩阵的乘积。
矩阵乘积C=AB中计算$C_{i,j}$的步骤看做A的第i行和B的第j列之间的点积。

## 矩阵性质
- 分配率
A(B+C) = AB+AC    （2.6）
- 结合律
A(BC) = (AB)C        （2.7）
- 转置性质
$(AB)^T = B^TA^T  （2.10）$  

注意，矩阵乘积不满足交换律，然而两个向量的点积满足交换律。

通常使用$Ax=b （2.11）$表示线性方程组。其中A是已知矩阵，b是已知向量，x是我们要求解的未知向量。


# 单位矩阵和逆矩阵
对于大多数矩阵A，我们通过矩阵逆解析地求解式（2.11).

## 单位矩阵 identity matrix
任意向量和单位矩阵相乘，都不会改变。
我们讲保持n维向量不变的单位矩阵记作$I_n$。
$$\forall x \in R^n, I_nx=x           (2.20)$$
单位矩阵的结构：所有沿主对角线的元素都是1，其他的位置的元素都是0。

## 逆矩阵
矩阵A的矩阵逆记作$A^{-1}$, 其定义的矩阵满足以下条件：
$$A^{-1}A=I_n  (2.21)$$
我们可以通过$x=A^{-1}b$来求解x。

# 线性相关和生成子空间
如果逆矩阵$A^{-1}$存在，那么式（2.11）中肯定对于每个向量b**恰好存在一个解**。
如果x和y都是某个方程组的解，则：
$$z=ax+(1+a)y(2.26)$$
其中$a$取任意实数，z也是方程组的解。
> 也就是说，如果有两个解，那么就有无穷多个解。

为了分析方程有多少个解，我们可以将A的列向量看做从原点（origin）出发的不同方向，确定有多少种方法可以到达向量b。在这个观点下，向量x中的每个元素表示我们应该沿着这个方向走多远，即$x_i$表示我们需要沿着第i个向量的方向走多远。
## 线性组合
我们称$Ax=\sum_ix_iA_{:,i}（2.27）$为**线性组合（linear combination）**。
形式上，一组向量的线性组合，是指每个向量乘以对应标量系数之后的和。
一组向量的 **生成子空间(span)** 是原始向量线性组合后所能抵达的点的集合。

所以确定$Ax=b$是否有解，相当于确定向量b是否在A列向量的生成子空间中。
$Ax=b$有解的一个基础条件是$n\geqslant m$，其中n是矩阵A的行数 ，m是矩阵A的列数。

如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为**线性无关（linearly independent）**。

如果我们想要对于**每个b值至多有一个解**。我们需要保证矩阵至多有m个列向量，$n\geqslant m$。否则，方程会有不止一个解。
## 奇异
**综上所述**，意味着矩阵必须是一个方阵（square）,即m=n，并且所有列向量都是线性无关的。一个列向量线性相关的方阵被称为**奇异的（singular）**

如果矩阵A不是一个方阵或者是一个奇异的方阵，则该方程仍然可能有解，但不是再使用逆矩阵求解。

# 范数
在机器学习中，我们经常用称为**范数（norm)** 的函数来衡量向量的大小。
形式上，$L^p$范数定义如下：
$$\left \| x \right \|_p=(\sum_i\left |x_i  \right |^p)^{\frac{1}{p}}  (2.30)$$
其中p大于等于1.

范数是将**向量映射到非负值的函数**。
直观上来说，向量x的范数衡量从原点到点x的距离。更严格的说，范数是满足以下性质的任意函数。
- $f(x) = 0 则 x=0$
- $f(x+y) \leqslant f(x)+f(y)$(三角不等式triangle inequality)

## $L^2$范数
当p=2时，$L^2$范数称为**欧几里得范数（Euclidean norm）**，表示从原点出发到向量x缺点的点的欧几里得距离。经常省略记为$\left \| x \right \|$。

## $L^1$范数
当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用$L^1$范数。每当x中某个元素从0增加$\epsilon$, 对应的$L^1$范数也会增加$\epsilon$。

## $L^0$范数
当需要统计向量中非零元素的个数来衡量向量的大小时，我们用到$L^0$范数，但是向量的非零元素的数据不是范数。所以$L^1$范数经常用作表示非零元素数目的替代函数。

## $L^\infty$范数
$L^\infty$范数被称为**最大范数（max norm）** 在机器学习中也经常出现。这个范数用来表示向量中具有最大幅值的元素的绝对值：
$$\left \| x \right \|_\infty = \max_i \left | x_i \right |(2.32)$$

## Frobeniusfan范数
在深度学习中，我们采用**Frobeniusfan范数**来衡量矩阵的大小。即
$$\left \| A\right\|_F = \sqrt{\sum_{i,j}A_{i,j}^2}$$
其类似于向量的$L^2$范数。

两个向量的点积可以用范数来表示，具体如下：
$$x^Ty = \left \| x \right\|_2 \left \| y \right\|_2 \cos\theta (2.34)$$
其中$\theta$表示x和y之间的夹角。

# 特殊类型的矩阵和向量
## 对角矩阵diagonal matrix
对角矩阵只在主对角线上含有非零元素，其他位置都是零。
我们用diag(v)表示对角元素由向量v中元素给定的一个对角方阵。

对于计算diag(v)x，我们只需要将x中的每个元素$x_i$放大$v_i$倍。
换言之，$diag(v)x = v \bigodot x$。

对角方阵的逆矩阵存在，当且仅当对角元素都是非零值的情况。
并非所有的对角矩阵都是方阵，但只有方阵对角矩阵才有逆矩阵。

## 对称矩阵 symmetric matrix
对称矩阵是转置和自己相等的矩阵，即$A = A^T$。

## 单位向量 unit vector
单位向量 是具有**单位范数unit norm**的向量，即$\left \| x \right\|_2=1(2.36)$

## 正交矩阵 orthogonal matrix
正交矩阵指行向量和列向量是分别标准的正交的方阵。即
$$A^TA = AA^T = I(2.37)$$
这意味着$A^{-1} = A^T$.

正交矩阵收到关注是因为求逆计算代价小。

# 特征分解
特征分解（eigendecomposition）是使用最广的矩阵分解之一，即**将矩阵分解成一组特征向量和特征值。**
方阵A的**特征向量eigenvector**是指与A相乘后相当于对该向量进行放缩的非零向量v：
$$Av = \lambda v(2.39)$$
其中标量$\lambda$称为这个特征向量对应的**特征值eigenvalue**。

假设矩阵A有n个线性无关的特征向量{$v^{(1)},v^{(2)},...,v^{(n)}$},对应着特征值{$\lambda_1,\lambda_2,...,\lambda_n$}。我们讲特征向量连成一个矩阵，使得每一列是一个特征向量。类似的我们也可以将特征值连成一个向量。因此A的**特征分解**可以记作：
$$A = V diag(\lambda)V^{-1}   (2.40)$$

不是每个矩阵都可以分解成特征值和特征向量。
每个实对称矩阵都可以分解成实特征向量和实特征值：
$$A = Q \Lambda Q^T (2.41)$$
其中Q是A的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,j}$对应的特征向量是矩阵Q的第i列，记作$Q_{:,i}$.因此Q是正交矩阵，我们将A看做沿方向$v^{(i)}$延展$\lambda_i$倍的空间.图如所示。

## 正定positive definite
所有特征值都是整数的矩阵称为**正定矩阵**； 所有特征值都是非负数的举证称为**半正定矩阵。** 所有特征值都是负数的矩阵称为**负定矩阵**。


# 奇异值分解SVD
奇异值分解是将矩阵分解为**奇异向量**和**奇异值**。通过奇异值分解，我们可以得到一些与特征分解相同类型的信息。
奇异值分解的应用更广，因为每个实数矩阵都有一个奇异值分解，但不一定有特征分解。
非方阵的矩阵没有特征分解，这时只能使用奇异值分解。

奇异值分解将矩阵A分解成三个矩阵的乘积：
$$A = UDV^T(2.43)$$
假设A是一个$m \times n$的矩阵，那么U是一个$m \times m$的矩阵，D是一个$m\timesn$的矩阵，V是一个$n\times n$的矩阵。
矩阵U和V都定义为正交矩阵。
矩阵D定义为对角矩阵。
对角矩阵D对角线上的元素称为矩阵A的**奇异值（singular value）**。
矩阵U的列向量称为**左奇异向量。**
矩阵V的列向量被为**右奇异向量。**

## 奇异分解和特征分解的关联
Ade非零奇异值是______特征值的平方根，同时也是_____特征值的平方根。

# Moore-Penrose伪逆
矩阵A的伪逆定义为：
$$A^+ = \lim_{a \to 0} (A^TA+\alpha I)^{-1}A^T (2.46)$$
伪逆的计算公式如下：
$$A^+ = VD^+U^T (2.47)$$
对角矩阵D的伪逆$D^+$是其非零元素取倒数之后再转置得到的。
>对角矩阵转置后保持不变

当矩阵A的列数多于行数时，常使用伪逆求解线性方程。
当矩阵A的行数多余列数时，可能没有解。

# 迹运算
迹运算返回的是矩阵对角元素的和：
$$Tr(A) = \sum_iA_{i,i}(2.48)$$
迹运算提供了另一种描述矩阵Frobenius范数的方式：
$$\left\| A \right\|_F = \sqrt{Tr(AA^T)}(2.49)$$

迹运算在转置运算下是不变的。
> A的迹和A的转置的迹是相同的。
> A的对角元素和A的转置的对角元素相同。

多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相乘得到的迹是相同的。
$$Tr(ABC) = Tr(CAB) = Tr(BCA) (2.51)$$
当只有两个矩阵时，有$Tr(AB) = Tr(BA)$.
标量在做迹运算之后仍是它自己：$a = Tr(a)$。

# 行列式
矩阵A的行列式，记作$det(A)$，是将方阵A映射到实数的函数。
**行列式等于矩阵特征值的乘积。**
行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。
如果行列式是1，那么这个转换保持空间体积不变。



